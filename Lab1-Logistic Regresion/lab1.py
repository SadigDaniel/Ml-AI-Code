# -*- coding: utf-8 -*-
"""Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DMl84j4wxC4Te3Nf1HmTMFN5pyW4RLxJ
"""



"""Lab #1
First and Last Name: Daniel Sadig

Student ID: 500894225
"""

from matplotlib import axis
import pandas as pd;
import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

df = pd.read_csv('https://raw.githubusercontent.com/tofighi/MachineLearning/master/datasets/student_marks.csv')


print("---------- Lab1: Without Standardization ----------:")
print("Midterm:")
print(f"Mean {np.mean(df['Midterm mark'])}")
print(f"Standard Diviation {np.std(df['Midterm mark'])}") #This is the std of midterm mark
print("\n")

print("Final mark:")
print(f"Mean: {np.mean(df['Final mark'])}")
print(f"Standard Deviation:  + {np.std(df['Final mark'])}")

print("\n")
print("---------- Standardition ----------:")
object= StandardScaler()

scale = object.fit_transform(df) 


print(" \n")
m = -0.5
b = 0


X = df['Midterm mark']
y_act = df['Final mark']
def plot_graph(X, y_act, m, b):
  plt.plot(X, y_act, "or")
  plt.xticks(list(range(20, 75, 5)))
  x = np.linspace(20,75,5)
  plt.plot(x, (m*(x)+b), '-b', label='y=-0.5x+0');



  plt.xlabel("Midterm Marks")
  plt.ylabel("Final Marks")

  plt.title('Grade Distribution Graph') 


print("---------- Mean Squared error ----------:")
sum = 0
tmpx = df['Midterm mark']

print(f"{format((1/len(X)) * np.sum((y_act - m*X + b)**2), '.2f')}") 


def gradient_descent(num_iter, m, X, b, y_act):
  print("---------- Performing Gradient Descent ----------:")
  #alph = 0.1
  alph = 0.0001 #Learning Rate
  x = np.linspace(20,75,5)
  n = len(X)
  Err_arr = []

  for i in range(num_iter):
    Y_pred = m*X + b  # The current predicted value of Y  
    D_m = (-2/n)* np.sum(X*(y_act - Y_pred)) #Derrivitive with respecgt to m (theta 0)
    D_b = (-2/n) * np.sum(y_act - Y_pred) #Derrivative with respect to b (Theta 1)
    m = m - alph * D_m
    b = b - alph * D_b
    Err_arr.append((round((1/n) * np.sum((y_act - m*X + b)**2), 2))) ## MAE for each new value
  

  print (m, b)
  
  plt.plot(x, (m*(x)+b), '-r', label=f"y={format(m,'.1f')}x+{format(b,'.2f')}, iter={num_iter}");
  plt.legend(loc='upper left')
  plt.show()

  print("---------- Error Graph at each iteration ----------:")
  plt.plot(Err_arr, "or")

# plt.plot(x, (m*(x)+b), '-b', label='y=0.5x+0');

  plt.xlabel("Iteration Number")
  plt.ylabel("Error")

  plt.title(f'Error per Iteration iter = {num_iter}') 
  plt.show()



plot_graph(X, y_act, m, b)
gradient_descent(100, m, X, b, y_act)

# For number of iterations = 2000
plot_graph(X, y_act, m, b)
gradient_descent(2000, m, X, b, y_act)

from matplotlib import axis
import pandas as pd;
import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error

df = pd.read_csv('https://raw.githubusercontent.com/tofighi/MachineLearning/master/datasets/student_marks.csv')


print("---------- Lab1: With Standardization ----------:")
print("Midterm:")
print(f"Mean {np.mean(df['Midterm mark'])}")
print(f"Standard Diviation {np.std(df['Midterm mark'])}") #This is the std of midterm mark
print("\n")

print("Final mark:")
print(f"Mean: {np.mean(df['Final mark'])}")
print(f"Standard Deviation:  + {np.std(df['Final mark'])}")

print("\n")

X = df['Midterm mark']
y_act = df['Final mark']

print("---------- Standardition ----------:")
standrdized_x = (X - np.mean(df['Midterm mark']))/np.std(df['Midterm mark'])
print(standrdized_x)


standrdized_y = (y_act - np.mean(y_act))/np.std(y_act)
print(standrdized_y)

print(" \n")
m = -0.5
b = 0



def plot_graph(standrdized_x, standrdized_y, m, b):
  plt.plot(standrdized_x, standrdized_y, "or")
  x = np.linspace(-3,2)
  plt.plot(x, (m*(x)+b), '-b', label='y=-0.5x+0');



  plt.xlabel("Midterm Marks")
  plt.ylabel("Final Marks")

  plt.title('Grade Distribution Graph') 


print("---------- Mean Squared error ----------:")
sum = 0
tmpx = df['Midterm mark']

print(f"{format((1/len(X)) * np.sum((standrdized_y - m*standrdized_x + b)**2), '.2f')}") 


def gradient_descent(num_iter, m, X, b, y_act):
  print("---------- Performing Gradient Descent ----------:")
  alph = 0.0001 #Learning Rate
  x = np.linspace(-3,2)
  n = len(X)
  Err_arr = []

  for i in range(num_iter):
    Y_pred = m*X + b  # The current predicted value of Y  
    D_m = (-2/n)* np.sum(X*(y_act - Y_pred)) #Derrivitive with respecgt to m (theta 0)
    D_b = (-2/n) * np.sum(y_act - Y_pred) #Derrivative with respect to b (Theta 1)
    m = m - alph * D_m
    b = b - alph * D_b
    Err_arr.append(((1/n) * np.sum((y_act - m*X + b)**2), 2)) ## MAE for each new value
  

  print (m, b)
  
  plt.plot(x, (m*(x)+b), '-r', label=f"y={format(m,'.1f')}x+{format(b,'.2f')}, iter=100");
  plt.legend(loc='upper left')
  plt.show()

  print("---------- Error Graph at each iteration ----------:")
  plt.plot(Err_arr)

# plt.plot(x, (m*(x)+b), '-b', label='y=0.5x+0');

  plt.xlabel("Iteration Number")
  plt.ylabel("Error")

  plt.title(f'Error per Iteration iter = {num_iter}') 
  plt.show()



plot_graph(standrdized_x, standrdized_y, m, b)
gradient_descent(100, m, standrdized_x, b, standrdized_y)

# For number of iterations = 2000
plot_graph(standrdized_x, standrdized_y, m, b)
gradient_descent(2000, m, standrdized_x, b, standrdized_y)

"""**Lab 1 Descreption Box**

**Q1: Write your observation by changing the learning rate to 0.1. Include error plots while explaining your observation. **

The learning rate is a hyperparameter that dictates the step size of the model parameters. A higher learning rate means a  more aggressive change in the model parameter values. This means there will be a great difference between the old and new model parameters. However, with a larger learning rate, there is a risk of overshooting the model parameters, meaning you miss the optimal solution. Subsequently, a smaller learning rate converges slower but it provides a more accurate solution. You are more likely to hit the minimum point as opposed to the local minimum point.

For part 1 of this lab, we see that changing the learning rate from 0.0001 to 0.1 causes us to overshoot the correct value of the graph, thus resulting in an error on our graph. Figure 1, has a step size of 0.0001 and creates an accurate line of best fit. Whereas with a step size of 0.1 and overshoots the line of best fit, thus resulting in an error.

However, for the standardized graph, we see that the step size is too small and thus the model parameters are not properly fitted for our graph. with standardization graph shows the model parameters for a step size of 0.0001. If we change the step size to 0.1 it is apparent that the model parameters are better suited for this graph. 

From this experiment, it is evident that step size is a hyperparameter that varies for each use case. Therefore it is important to test different step sizes when running a linear regression model, this ensures that you create a better mathematical model with fewer epochs.


**Q2: Investigate the effect of standardization?**

Standardization transforms the inputs such that their mean and unit variance is equal to 0, this means that all features are on the same scale. This allows the algorithm to better optimize the solution by creating a more uniform cost function, which leads to a better solution. Also, by standardizing the features you reduce the variance of the scale of the graph, which means that there are fewer outlying data points. This makes it easier to perform optimization, especially when using mini-batch and other techniques. Since mini batch only considers a small size of features, if the chosen features vary greatly from the mean the resulting model parameters will not be accurate.

"""

